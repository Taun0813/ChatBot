# ===========================================
# AI AGENT CONFIGURATION - UPDATED STRUCTURE
# ===========================================

# Environment
ENVIRONMENT=development
DEBUG=true
LOG_LEVEL=INFO

# ===========================================
# FREE API KEYS (RECOMMENDED)
# ===========================================

# Google Gemini API (FREE) - Recommended
GEMINI_API_KEY=your_gemini_api_key_here
# Get from: https://makersuite.google.com/app/apikey

# Groq API (FREE) - High Performance
GROQ_API_KEY=your_groq_api_key_here
# Get from: https://console.groq.com/

# Ollama (FREE - Local) - Completely Free
OLLAMA_BASE_URL=http://localhost:11434
# Install from: https://ollama.ai/

# ===========================================
# OPTIONAL PAID API KEYS
# ===========================================

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Claude Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Cohere Configuration
COHERE_API_KEY=your_cohere_api_key_here

# Hugging Face Configuration
HUGGINGFACE_TOKEN=your_huggingface_token_here

# ===========================================
# PINECONE VECTOR DATABASE CONFIGURATION (REQUIRED)
# ===========================================

# Pinecone Configuration - BẮT BUỘC cho vector storage
PINECONE_API_KEY=your_pinecone_api_key_here
# Get from: https://app.pinecone.io/
PINECONE_ENVIRONMENT=us-west1-gcp-free
PINECONE_INDEX=ai-agent-index
PINECONE_DIMENSION=768
PINECONE_METRIC=cosine
PINECONE_NAMESPACE=default

# ===========================================
# BACKEND SELECTION - CLOUD ONLY
# ===========================================

# Vector Store Backend (chỉ Pinecone cloud)
VECTORSTORE_BACKEND=pinecone

# Model Loader Backend (gemini, groq, ollama, openai, anthropic, transformers)
MODEL_LOADER_BACKEND=gemini

# Lưu ý: Không còn hỗ trợ FAISS local, chỉ sử dụng Pinecone cloud

# ===========================================
# PERSONALIZATION CONFIGURATION
# ===========================================

# Enable Personalization Features
ENABLE_PERSONALIZATION=true
ENABLE_RECOMMENDATIONS=true
ENABLE_RL_LEARNING=true

# Personalization Directories
PROFILES_DIR=./data/profiles
MODELS_DIR=./data/models

# ===========================================
# SERVER CONFIGURATION
# ===========================================

API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1
API_RELOAD=false
CORS_ORIGINS=*
RATE_LIMIT=100
MAX_REQUEST_SIZE=1048576
DEVELOPMENT_MODE=true

# ===========================================
# CACHING CONFIGURATION
# ===========================================

# Cache Type (memory, redis)
CACHE_TYPE=memory

# Redis Configuration (if using Redis cache)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_MAX_CONNECTIONS=10

# Memory Cache Configuration
MEMORY_CACHE_MAX_SIZE=1000
MEMORY_CACHE_DEFAULT_TTL=3600
MEMORY_CACHE_CLEANUP_INTERVAL=300

# ===========================================
# MONITORING CONFIGURATION
# ===========================================

# Metrics Collection
ENABLE_METRICS=true
METRICS_EXPORT_INTERVAL=300
ENABLE_PROMETHEUS=false
PROMETHEUS_PORT=9090

# Health Checks
ENABLE_HEALTH_CHECKS=true
HEALTH_CHECK_INTERVAL=30
HEALTH_CHECK_TIMEOUT=10

# Request Tracing
ENABLE_TRACING=true
TRACE_EXPORT_INTERVAL=300

# System Thresholds
MEMORY_THRESHOLD=90.0
CPU_THRESHOLD=90.0
DISK_THRESHOLD=90.0

# ===========================================
# MODEL CONFIGURATION
# ===========================================

# Model Paths
TINYLLAMA_BASE_PATH=F:/hf_cache/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0
TINYLLAMA_FINETUNED_PATH=./models/tinyllama-retail-finetuned
EMBEDDING_MODEL=F:/hf_cache/hub/models--intfloat--multilingual-e5-base
MODELS_CACHE_DIR=F:/hf_cache

# Model Parameters
MODEL_MAX_LENGTH=512
MODEL_TEMPERATURE=0.7
MODEL_TOP_P=0.9
MODEL_REPETITION_PENALTY=1.1
LOAD_IN_4BIT=true
MODEL_DEVICE=auto
TRUST_REMOTE_CODE=true

# ===========================================
# RAG CONFIGURATION
# ===========================================

RAG_SEARCH_TOP_K=5
RAG_SCORE_THRESHOLD=0.7
RAG_MAX_QUERY_LENGTH=200
RAG_ENABLE_RERANKING=false
RAG_CHUNK_SIZE=500
RAG_CHUNK_OVERLAP=50

# ===========================================
# CHAT CONFIGURATION
# ===========================================

CHAT_MAX_HISTORY=10
CHAT_SESSION_TIMEOUT=3600
CHAT_ENABLE_MEMORY=true
CHAT_MAX_SESSIONS=1000

# ===========================================
# DATABASE CONFIGURATION
# ===========================================

DATABASE_URL=sqlite:///./ai_agent.db

# ===========================================
# TRAINING CONFIGURATION
# ===========================================

TRAINING_DATASET_PATH=./training/dataset
TRAINING_OUTPUT_DIR=./models/tinyllama-retail-finetuned
TRAINING_EPOCHS=3
TRAINING_BATCH_SIZE=4
GRADIENT_ACCUMULATION=4
LEARNING_RATE=2e-4
MAX_SEQ_LENGTH=512
WARMUP_STEPS=100
LOGGING_STEPS=25
SAVE_STEPS=500
EVAL_STEPS=500
LORA_R=16
LORA_ALPHA=32
LORA_DROPOUT=0.1

# ===========================================
# DATA CONFIGURATION - UPDATED
# ===========================================

# Dataset Configuration
DATASET_FILE=./training/dataset/dataset.json
PRODUCTS_DATA_FILE=./data/processed/products.json
CONVERSATIONS_DATA_FILE=./data/processed/conversations.json
KNOWLEDGE_BASE_FILE=./data/processed/knowledge_base.json
TRAINING_DATA_FILE=./data/processed/training_data.json

# Data Processing
DATA_BATCH_SIZE=100
EMBEDDING_BATCH_SIZE=32
DATA_MAX_RETRIES=3
DATA_RETRY_DELAY=1

# Data Directories
PROCESSED_DATA_DIR=./data/processed
PROFILES_DATA_DIR=./data/profiles
MODELS_DATA_DIR=./data/models

# ===========================================
# EXTERNAL SERVICES
# ===========================================

# Order Service
ORDER_SERVICE_URL=http://localhost:8001
ORDER_SERVICE_API_KEY=your_order_service_api_key
ORDER_SERVICE_TIMEOUT=30

# Product Service
PRODUCT_SERVICE_URL=http://localhost:8002
PRODUCT_SERVICE_API_KEY=your_product_service_api_key
PRODUCT_SERVICE_TIMEOUT=30

# Payment Service
PAYMENT_SERVICE_URL=http://localhost:8003
PAYMENT_SERVICE_API_KEY=your_payment_service_api_key
PAYMENT_SERVICE_TIMEOUT=30

# Warranty Service
WARRANTY_SERVICE_URL=http://localhost:8004
WARRANTY_SERVICE_API_KEY=your_warranty_service_api_key
WARRANTY_SERVICE_TIMEOUT=30

# ===========================================
# PERFORMANCE CONFIGURATION
# ===========================================

ENABLE_CACHING=true
CACHE_TTL=3600
MAX_CACHE_SIZE=1000
ENABLE_ASYNC=true
MAX_CONCURRENT_REQUESTS=10
REQUEST_TIMEOUT=60

# ===========================================
# SECURITY CONFIGURATION
# ===========================================

ALLOWED_API_KEYS=
ENABLE_AUTH=false
JWT_SECRET=your-secret-key-change-in-production
JWT_ALGORITHM=HS256
JWT_EXPIRATION=3600
ALLOWED_IPS=
RATE_LIMIT_ENABLED=true

# ===========================================
# AGENT ORCHESTRATOR CONFIGURATION
# ===========================================

# Orchestrator Settings
ENABLE_PARALLEL_PROCESSING=true
MAX_CONCURRENT_AGENTS=3
DEFAULT_AGENT_TIMEOUT=30
ENABLE_FALLBACK=true
FALLBACK_AGENT=conversation

# Agent Configuration
RAG_AGENT_ENABLED=true
CONVERSATION_AGENT_ENABLED=true
API_AGENT_ENABLED=true

# ===========================================
# QUICK START GUIDE - CLOUD ONLY
# ===========================================

# 1. Copy this file to .env
# 2. Get free API keys:
#    - Gemini: https://makersuite.google.com/app/apikey
#    - Groq: https://console.groq.com/
#    - Ollama: https://ollama.ai/ (install locally)
#    - Pinecone: https://app.pinecone.io/ (BẮT BUỘC)
# 3. Set MODEL_LOADER_BACKEND=gemini (or groq, ollama)
# 4. Set PINECONE_API_KEY=your_key_here (BẮT BUỘC)
# 5. Run: python app.py
# 6. Test: curl http://localhost:8000/health

# LƯU Ý: Hệ thống chỉ sử dụng Pinecone cloud, không hỗ trợ FAISS local

# ===========================================
# LOGGING CONFIGURATION
# ===========================================

LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s
LOG_FILE=
